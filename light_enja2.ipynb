{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "light_enja2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lt7aty4qSRW"
      },
      "source": [
        "日->英 翻訳器をつくります．\n",
        "\n",
        "まずは必要なデータやライブラリをそろえます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RmV_ca5uafqf",
        "outputId": "c9bc3906-8509-4e1b-eef4-6aafc5a69ad1"
      },
      "source": [
        "! git clone https://github.com/nymwa/light_enja2.git\n",
        "! mv light_enja2/* .\n",
        "! pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'light_enja2'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 30 (delta 4), reused 30 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n",
            "Collecting sacremoses==0.0.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 30.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.9.0+cu102)\n",
            "Collecting fairseq==0.10.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/ab/92c6efb05ffdfe16fbdc9e463229d9af8c3b74dc943ed4b4857a87b223c2/fairseq-0.10.2-cp37-cp37m-manylinux1_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 46.5MB/s \n",
            "\u001b[?25hCollecting sacrebleu==1.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.1MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.96\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.35->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.35->-r requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.35->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.35->-r requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==0.10.2->-r requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==0.10.2->-r requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.10.2->-r requirements.txt (line 3)) (0.29.23)\n",
            "Collecting hydra-core\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/cd/85aa2e3a8babc36feac99df785e54abf99afbc4acc20488630f3ef46980a/hydra_core-1.1.0-py3-none-any.whl (144kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 49.1MB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.10.2->-r requirements.txt (line 3)) (1.14.6)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 57.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq==0.10.2->-r requirements.txt (line 3)) (5.2.0)\n",
            "Collecting omegaconf==2.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/96/1966b48bfe6ca64bfadfa7bcc9a8d73c5d83b4be769321fcc5d617abeb0c/omegaconf-2.1.0-py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.10.2->-r requirements.txt (line 3)) (2.20)\n",
            "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq==0.10.2->-r requirements.txt (line 3)) (3.5.0)\n",
            "Collecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 49.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: sacremoses, antlr4-python3-runtime\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp37-none-any.whl size=883990 sha256=1572d7087702ed4771c6b586f584c0cb4056815914ba58f1b8b2369dde9871bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=616a8f8f39b587b8f26d4b6c34568c73c1e5e63eefc6e7cd68a1971b6eaeb7a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "Successfully built sacremoses antlr4-python3-runtime\n",
            "Installing collected packages: sacremoses, portalocker, sacrebleu, antlr4-python3-runtime, PyYAML, omegaconf, hydra-core, dataclasses, fairseq, sentencepiece\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 dataclasses-0.6 fairseq-0.10.2 hydra-core-1.1.0 omegaconf-2.1.0 portalocker-2.0.0 sacrebleu-1.5.1 sacremoses-0.0.35 sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgFZmYSHqdld"
      },
      "source": [
        "データに対して前処理を行います\n",
        "\n",
        "ダウンロードした時点でデータセットは単語分割やトークン化がすでに施されています．\n",
        "マージ数8000のBPEでサブワードに分割します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VQQIdbqaxkJ",
        "outputId": "cd06399e-3d17-43bd-ece1-7e918340203d"
      },
      "source": [
        "! bash preproc.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+ TRAIN_EN=corpus/train.en\n",
            "+ TRAIN_JA=corpus/train.ja\n",
            "+ VALID_EN=corpus/valid.en\n",
            "+ VALID_JA=corpus/valid.ja\n",
            "+ TEST_EN=corpus/test.en\n",
            "+ cat corpus/train.en corpus/train.ja\n",
            "+ python src/learn.py --input train.enja --prefix bpe --vocab-size 4000 --character-coverage 0.9995 --threads 1\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: train.enja\n",
            "  input_format: \n",
            "  model_prefix: bpe\n",
            "  model_type: BPE\n",
            "  vocab_size: 4000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 1\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(178) LOG(INFO) Loading corpus: train.enja\n",
            "trainer_interface.cc(385) LOG(INFO) Loaded all 200000 sentences\n",
            "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(466) LOG(INFO) all chars count=6286314\n",
            "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
            "trainer_interface.cc(487) LOG(INFO) Alphabet size=1546\n",
            "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
            "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 200000 sentences.\n",
            "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 200000\n",
            "trainer_interface.cc(537) LOG(INFO) Done! 16208\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=110415 min_freq=268\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=31872 size=20 all=11962 active=1291 piece=in\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19776 size=40 all=12337 active=1666 piece=is\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14446 size=60 all=12924 active=2253 piece=▁と\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9019 size=80 all=13419 active=2748 piece=▁we\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7250 size=100 all=13708 active=3037 piece=▁on\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=7233 min_freq=474\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5610 size=120 all=14035 active=1322 piece=▁have\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4509 size=140 all=14435 active=1722 piece=os\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3861 size=160 all=14773 active=2060 piece=▁ある\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3339 size=180 all=15200 active=2487 piece=ea\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2840 size=200 all=15402 active=2689 piece=▁whe\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2837 min_freq=438\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2537 size=220 all=15746 active=1340 piece=▁それ\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2304 size=240 all=16056 active=1650 piece=ck\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2115 size=260 all=16333 active=1927 piece=ame\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1953 size=280 all=16635 active=2229 piece=▁en\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1837 size=300 all=16838 active=2432 piece=▁会\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1835 min_freq=382\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1708 size=320 all=17092 active=1236 piece=▁!\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1613 size=340 all=17236 active=1380 piece=▁わ\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1474 size=360 all=17401 active=1545 piece=▁好き\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1402 size=380 all=17560 active=1704 piece=▁もう\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1314 size=400 all=17757 active=1901 piece=ild\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1313 min_freq=347\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1249 size=420 all=17875 active=1107 piece=▁自分\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1199 size=440 all=18062 active=1294 piece=▁off\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1134 size=460 all=18227 active=1459 piece=itt\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1098 size=480 all=18323 active=1555 piece=▁時間\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1040 size=500 all=18479 active=1711 piece=▁車\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1039 min_freq=310\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1008 size=520 all=18622 active=1142 piece=▁テ\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=979 size=540 all=18735 active=1255 piece=▁分\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=933 size=560 all=18818 active=1338 piece=▁friend\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=895 size=580 all=18995 active=1515 piece=ove\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=855 size=600 all=19102 active=1622 piece=own\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=852 min_freq=287\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=834 size=620 all=19239 active=1128 piece=▁明日\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=804 size=640 all=19349 active=1238 piece=▁plan\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=786 size=660 all=19437 active=1326 piece=▁本当\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=766 size=680 all=19509 active=1398 piece=▁同\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=740 size=700 all=19607 active=1496 piece=▁boy\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=739 min_freq=262\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=709 size=720 all=19734 active=1126 piece=ark\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=692 size=740 all=19814 active=1206 piece=▁week\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=668 size=760 all=19981 active=1373 piece=しく\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=645 size=780 all=20146 active=1538 piece=▁gr\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=622 size=800 all=20312 active=1704 piece=▁金\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=622 min_freq=242\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=604 size=820 all=20431 active=1127 piece=able\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=584 size=840 all=20540 active=1236 piece=▁聞い\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=564 size=860 all=20660 active=1356 piece=▁nothing\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=545 size=880 all=20738 active=1434 piece=▁働\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=526 size=900 all=20866 active=1562 piece=rest\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=525 min_freq=219\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=504 size=920 all=20955 active=1125 piece=▁count\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=491 size=940 all=21071 active=1241 piece=bod\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=482 size=960 all=21232 active=1402 piece=▁世\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=470 size=980 all=21365 active=1535 piece=▁news\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=457 size=1000 all=21444 active=1614 piece=ting\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=457 min_freq=200\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=447 size=1020 all=21514 active=1130 piece=▁days\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=437 size=1040 all=21568 active=1184 piece=▁パー\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=426 size=1060 all=21658 active=1274 piece=ope\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=417 size=1080 all=21729 active=1345 piece=cle\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=411 size=1100 all=21773 active=1389 piece=▁boston\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=411 min_freq=188\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=401 size=1120 all=21838 active=1154 piece=ible\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=390 size=1140 all=21968 active=1284 piece=ause\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=384 size=1160 all=22062 active=1378 piece=▁meeting\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=376 size=1180 all=22146 active=1462 piece=igh\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=371 size=1200 all=22235 active=1551 piece=▁切\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=371 min_freq=174\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=365 size=1220 all=22329 active=1197 piece=▁夜\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=359 size=1240 all=22356 active=1224 piece=▁会い\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=351 size=1260 all=22424 active=1292 piece=▁難\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=346 size=1280 all=22507 active=1375 piece=▁くる\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=338 size=1300 all=22571 active=1439 piece=ip\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=338 min_freq=163\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=329 size=1320 all=22646 active=1184 piece=▁うま\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=320 size=1340 all=22750 active=1288 piece=▁6\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=313 size=1360 all=22779 active=1317 piece=▁眠\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=309 size=1380 all=22826 active=1364 piece=ニス\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=305 size=1400 all=22905 active=1443 piece=▁alone\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=304 min_freq=152\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=299 size=1420 all=22982 active=1223 piece=▁food\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=294 size=1440 all=23017 active=1258 piece=▁場\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=290 size=1460 all=23102 active=1343 piece=▁afraid\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=285 size=1480 all=23154 active=1395 piece=tain\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=281 size=1500 all=23210 active=1451 piece=oon\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=281 min_freq=143\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=278 size=1520 all=23249 active=1194 piece=▁supp\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=274 size=1540 all=23328 active=1273 piece=▁both\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=269 size=1560 all=23429 active=1374 piece=uth\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=266 size=1580 all=23479 active=1424 piece=▁large\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=261 size=1600 all=23536 active=1481 piece=▁死ん\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=261 min_freq=134\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=256 size=1620 all=23563 active=1204 piece=▁座っ\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=254 size=1640 all=23613 active=1254 piece=lain\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=251 size=1660 all=23672 active=1313 piece=▁知り\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=247 size=1680 all=23719 active=1360 piece=▁コン\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=243 size=1700 all=23787 active=1428 piece=▁意見\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=243 min_freq=125\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=240 size=1720 all=23841 active=1244 piece=▁足\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=237 size=1740 all=23893 active=1296 piece=▁free\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=232 size=1760 all=23923 active=1326 piece=▁願\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=229 size=1780 all=23978 active=1381 piece=▁速\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=226 size=1800 all=24025 active=1428 piece=▁short\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=226 min_freq=119\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=223 size=1820 all=24076 active=1250 piece=▁ele\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=220 size=1840 all=24134 active=1308 piece=▁then\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=217 size=1860 all=24166 active=1340 piece=っかり\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=214 size=1880 all=24208 active=1382 piece=▁読む\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=211 size=1900 all=24247 active=1421 piece=▁終わっ\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=211 min_freq=111\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=209 size=1920 all=24293 active=1259 piece=▁result\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=205 size=1940 all=24345 active=1311 piece=▁馬\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=203 size=1960 all=24381 active=1347 piece=▁日曜日\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=200 size=1980 all=24431 active=1397 piece=▁結果\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=197 size=2000 all=24451 active=1417 piece=▁靴\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=197 min_freq=106\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=195 size=2020 all=24488 active=1257 piece=▁cam\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=193 size=2040 all=24535 active=1304 piece=▁number\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=190 size=2060 all=24575 active=1344 piece=▁bor\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=188 size=2080 all=24623 active=1392 piece=ross\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=185 size=2100 all=24643 active=1412 piece=▁あい\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=185 min_freq=100\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=183 size=2120 all=24674 active=1259 piece=▁uncle\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=181 size=2140 all=24716 active=1301 piece=▁others\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=179 size=2160 all=24763 active=1347 piece=form\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=177 size=2180 all=24827 active=1411 piece=▁おも\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=175 size=2200 all=24849 active=1433 piece=▁making\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=174 min_freq=95\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=173 size=2220 all=24874 active=1268 piece=▁持ち\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=171 size=2240 all=24916 active=1310 piece=▁box\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=169 size=2260 all=24942 active=1336 piece=▁怒っ\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=168 size=2280 all=24977 active=1371 piece=▁ter\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=166 size=2300 all=25035 active=1429 piece=here\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=166 min_freq=91\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=164 size=2320 all=25058 active=1272 piece=▁賛成\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=162 size=2340 all=25086 active=1300 piece=▁外出\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=161 size=2360 all=25122 active=1336 piece=▁テーブル\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=159 size=2380 all=25147 active=1361 piece=▁passed\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=157 size=2400 all=25213 active=1427 piece=▁paper\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=157 min_freq=87\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=155 size=2420 all=25251 active=1299 piece=▁account\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=152 size=2440 all=25275 active=1323 piece=▁招\n",
            "trainer_interface.cc(615) LOG(INFO) Saving model: bpe.model\n",
            "trainer_interface.cc(626) LOG(INFO) Saving vocabs: bpe.vocab\n",
            "+ encode\n",
            "+ python src/encode.py --model bpe.model\n",
            "+ encode\n",
            "+ python src/encode.py --model bpe.model\n",
            "+ encode\n",
            "+ python src/encode.py --model bpe.model\n",
            "+ encode\n",
            "+ python src/encode.py --model bpe.model\n",
            "+ encode\n",
            "+ python src/encode.py --model bpe.model\n",
            "+ fairseq-preprocess -s en -t ja --trainpref train --validpref valid --destdir data-bin --joined-dictionary\n",
            "2021-07-17 12:49:57 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='en', srcdict=None, target_lang='ja', task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='train', user_dir=None, validpref='valid', workers=1)\n",
            "2021-07-17 12:50:07 | INFO | fairseq_cli.preprocess | [en] Dictionary: 3888 types\n",
            "2021-07-17 12:50:12 | INFO | fairseq_cli.preprocess | [en] train.en: 100000 sents, 1149344 tokens, 0.0% replaced by <unk>\n",
            "2021-07-17 12:50:12 | INFO | fairseq_cli.preprocess | [en] Dictionary: 3888 types\n",
            "2021-07-17 12:50:12 | INFO | fairseq_cli.preprocess | [en] valid.en: 5000 sents, 57553 tokens, 0.0% replaced by <unk>\n",
            "2021-07-17 12:50:12 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 3888 types\n",
            "2021-07-17 12:50:18 | INFO | fairseq_cli.preprocess | [ja] train.ja: 100000 sents, 1367026 tokens, 0.0% replaced by <unk>\n",
            "2021-07-17 12:50:18 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 3888 types\n",
            "2021-07-17 12:50:18 | INFO | fairseq_cli.preprocess | [ja] valid.ja: 5000 sents, 68289 tokens, 0.0% replaced by <unk>\n",
            "2021-07-17 12:50:18 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr70ETxTq2Pg"
      },
      "source": [
        "学習を行います．\n",
        "\n",
        "少ないデータでかつかなり軽量なモデルでの学習ですが，10分ぐらいかかります．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VyE7idzbARO",
        "outputId": "070bd40c-834b-4112-8fb5-5632998ab2d6"
      },
      "source": [
        "!fairseq-train \\\n",
        "    data-bin \\\n",
        "    --save-interval 10 \\\n",
        "    --max-epoch 10 \\\n",
        "    --update-freq 1 \\\n",
        "    --max-tokens 4000 \\\n",
        "    --arch transformer \\\n",
        "    --encoder-normalize-before \\\n",
        "    --decoder-normalize-before \\\n",
        "    --encoder-embed-dim 512 \\\n",
        "    --encoder-ffn-embed-dim 1024 \\\n",
        "    --encoder-attention-heads 4 \\\n",
        "    --encoder-layers 4 \\\n",
        "    --decoder-embed-dim 512 \\\n",
        "    --decoder-ffn-embed-dim 1024 \\\n",
        "    --decoder-attention-heads 4 \\\n",
        "    --decoder-layers 4 \\\n",
        "    --share-all-embeddings \\\n",
        "    --dropout 0.3 \\\n",
        "    --optimizer adam \\\n",
        "    --adam-betas '(0.9, 0.999)' \\\n",
        "    --lr 0.002 \\\n",
        "    --lr-scheduler inverse_sqrt \\\n",
        "    --warmup-updates 2000 \\\n",
        "    --warmup-init-lr 1e-07 \\\n",
        "    --clip-norm 1.0 \\\n",
        "    --weight-decay 0.01 \\\n",
        "    --criterion label_smoothed_cross_entropy \\\n",
        "    --label-smoothing 0.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-17 12:50:41 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=4, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=4, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.6, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.002], lr_scheduler='inverse_sqrt', max_epoch=10, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=10, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=2000, weight_decay=0.01, zero_sharding='none')\n",
            "2021-07-17 12:50:41 | INFO | fairseq.tasks.translation | [en] dictionary: 3888 types\n",
            "2021-07-17 12:50:41 | INFO | fairseq.tasks.translation | [ja] dictionary: 3888 types\n",
            "2021-07-17 12:50:41 | INFO | fairseq.data.data_utils | loaded 5000 examples from: data-bin/valid.en-ja.en\n",
            "2021-07-17 12:50:41 | INFO | fairseq.data.data_utils | loaded 5000 examples from: data-bin/valid.en-ja.ja\n",
            "2021-07-17 12:50:41 | INFO | fairseq.tasks.translation | data-bin valid en-ja 5000 examples\n",
            "2021-07-17 12:50:42 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(3888, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(3888, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=512, out_features=3888, bias=False)\n",
            "  )\n",
            ")\n",
            "2021-07-17 12:50:42 | INFO | fairseq_cli.train | task: translation (TranslationTask)\n",
            "2021-07-17 12:50:42 | INFO | fairseq_cli.train | model: transformer (TransformerModel)\n",
            "2021-07-17 12:50:42 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)\n",
            "2021-07-17 12:50:42 | INFO | fairseq_cli.train | num. model params: 23021568 (num. trained: 23021568)\n",
            "2021-07-17 12:50:53 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
            "2021-07-17 12:50:53 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2021-07-17 12:50:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-07-17 12:50:53 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
            "2021-07-17 12:50:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-07-17 12:50:53 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2021-07-17 12:50:53 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None\n",
            "2021-07-17 12:50:53 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n",
            "2021-07-17 12:50:53 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2021-07-17 12:50:53 | INFO | fairseq.data.data_utils | loaded 100000 examples from: data-bin/train.en-ja.en\n",
            "2021-07-17 12:50:53 | INFO | fairseq.data.data_utils | loaded 100000 examples from: data-bin/train.en-ja.ja\n",
            "2021-07-17 12:50:53 | INFO | fairseq.tasks.translation | data-bin train en-ja 100000 examples\n",
            "2021-07-17 12:50:53 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\n",
            "epoch 001:   0% 0/368 [00:00<?, ?it/s]2021-07-17 12:50:53 | INFO | fairseq.trainer | begin training epoch 1\n",
            "/usr/local/lib/python3.7/dist-packages/fairseq/utils.py:342: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n",
            "epoch 001: 100% 367/368 [01:08<00:00,  5.58it/s, loss=10.436, nll_loss=7.364, ppl=164.78, wps=19673.6, ups=5.31, wpb=3706.5, bsz=263, num_updates=300, lr=0.000300085, gnorm=0.385, clip=0, train_wall=19, wall=56]2021-07-17 12:52:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 1/25 [00:00<00:02,  9.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 3/25 [00:00<00:01, 11.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 5/25 [00:00<00:01, 12.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  28% 7/25 [00:00<00:01, 13.81it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 9/25 [00:00<00:01, 14.77it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 15.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 13/25 [00:00<00:00, 15.74it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 16.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 17/25 [00:01<00:00, 15.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 19/25 [00:01<00:00, 16.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  84% 21/25 [00:01<00:00, 16.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 23/25 [00:01<00:00, 16.73it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-07-17 12:52:03 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.185 | nll_loss 6.126 | ppl 69.84 | wps 47543 | wpb 2731.6 | bsz 200 | num_updates 368\n",
            "2021-07-17 12:52:03 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2021-07-17 12:52:03 | INFO | train | epoch 001 | loss 10.923 | nll_loss 8.426 | ppl 343.92 | wps 19667.6 | ups 5.29 | wpb 3714.7 | bsz 271.7 | num_updates 368 | lr 0.000368082 | gnorm 0.781 | clip 10.9 | train_wall 68 | wall 70\n",
            "epoch 002:   0% 0/368 [00:00<?, ?it/s]2021-07-17 12:52:03 | INFO | fairseq.trainer | begin training epoch 2\n",
            "epoch 002: 100% 367/368 [01:11<00:00,  5.28it/s, loss=9.75, nll_loss=5.363, ppl=41.14, wps=18707.7, ups=5.11, wpb=3657.6, bsz=267.4, num_updates=700, lr=0.000700065, gnorm=0.347, clip=0, train_wall=19, wall=134]2021-07-17 12:53:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 1/25 [00:00<00:02,  8.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 3/25 [00:00<00:02, 10.48it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 5/25 [00:00<00:01, 11.81it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  28% 7/25 [00:00<00:01, 13.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  36% 9/25 [00:00<00:01, 14.18it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 14.73it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  52% 13/25 [00:00<00:00, 15.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 15.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 17/25 [00:01<00:00, 15.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  76% 19/25 [00:01<00:00, 15.72it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  84% 21/25 [00:01<00:00, 16.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 23/25 [00:01<00:00, 16.37it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-07-17 12:53:16 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.573 | nll_loss 4.598 | ppl 24.22 | wps 46481.2 | wpb 2731.6 | bsz 200 | num_updates 736\n",
            "2021-07-17 12:53:16 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2021-07-17 12:53:16 | INFO | train | epoch 002 | loss 9.887 | nll_loss 5.763 | ppl 54.3 | wps 18777.9 | ups 5.05 | wpb 3714.7 | bsz 271.7 | num_updates 736 | lr 0.000736063 | gnorm 0.37 | clip 0 | train_wall 71 | wall 143\n",
            "epoch 003:   0% 0/368 [00:00<?, ?it/s]2021-07-17 12:53:16 | INFO | fairseq.trainer | begin training epoch 3\n",
            "epoch 003: 100% 367/368 [01:12<00:00,  5.00it/s, loss=9.42, nll_loss=4.459, ppl=22, wps=18856.2, ups=5.06, wpb=3729.9, bsz=272.7, num_updates=1100, lr=0.00110005, gnorm=0.292, clip=0, train_wall=20, wall=215]2021-07-17 12:54:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   4% 1/25 [00:00<00:02,  9.49it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  12% 3/25 [00:00<00:02, 11.00it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  20% 5/25 [00:00<00:01, 12.25it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  28% 7/25 [00:00<00:01, 13.53it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  36% 9/25 [00:00<00:01, 14.43it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 14.86it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  52% 13/25 [00:00<00:00, 15.31it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 15.11it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  68% 17/25 [00:01<00:00, 15.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  76% 19/25 [00:01<00:00, 15.56it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  84% 21/25 [00:01<00:00, 16.24it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  92% 23/25 [00:01<00:00, 16.04it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-07-17 12:54:30 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.349 | nll_loss 3.844 | ppl 14.36 | wps 45793.3 | wpb 2731.6 | bsz 200 | num_updates 1104\n",
            "2021-07-17 12:54:30 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2021-07-17 12:54:30 | INFO | train | epoch 003 | loss 9.494 | nll_loss 4.655 | ppl 25.2 | wps 18420 | ups 4.96 | wpb 3714.7 | bsz 271.7 | num_updates 1104 | lr 0.00110404 | gnorm 0.302 | clip 0 | train_wall 72 | wall 217\n",
            "epoch 004:   0% 0/368 [00:00<?, ?it/s]2021-07-17 12:54:30 | INFO | fairseq.trainer | begin training epoch 4\n",
            "epoch 004: 100% 367/368 [01:13<00:00,  5.20it/s, loss=9.306, nll_loss=4.165, ppl=17.94, wps=18471.3, ups=4.94, wpb=3739, bsz=268.7, num_updates=1400, lr=0.00140003, gnorm=0.234, clip=0, train_wall=20, wall=276]2021-07-17 12:55:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   4% 1/25 [00:00<00:02,  9.39it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  12% 3/25 [00:00<00:02, 10.71it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  20% 5/25 [00:00<00:01, 11.89it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  28% 7/25 [00:00<00:01, 13.07it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  36% 9/25 [00:00<00:01, 13.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 14.41it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  52% 13/25 [00:00<00:00, 14.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 14.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  68% 17/25 [00:01<00:00, 15.01it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  76% 19/25 [00:01<00:00, 15.51it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  84% 21/25 [00:01<00:00, 16.13it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  92% 23/25 [00:01<00:00, 15.86it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-07-17 12:55:46 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.204 | nll_loss 3.576 | ppl 11.93 | wps 44810.5 | wpb 2731.6 | bsz 200 | num_updates 1472\n",
            "2021-07-17 12:55:46 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2021-07-17 12:55:46 | INFO | train | epoch 004 | loss 9.315 | nll_loss 4.187 | ppl 18.22 | wps 18097.4 | ups 4.87 | wpb 3714.7 | bsz 271.7 | num_updates 1472 | lr 0.00147203 | gnorm 0.248 | clip 0 | train_wall 73 | wall 292\n",
            "epoch 005:   0% 0/368 [00:00<?, ?it/s]2021-07-17 12:55:46 | INFO | fairseq.trainer | begin training epoch 5\n",
            "epoch 005: 100% 367/368 [01:14<00:00,  5.04it/s, loss=9.2, nll_loss=3.904, ppl=14.97, wps=18445, ups=4.94, wpb=3733.2, bsz=270.6, num_updates=1800, lr=0.00180001, gnorm=0.2, clip=0, train_wall=20, wall=359]2021-07-17 12:57:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   4% 1/25 [00:00<00:02,  8.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  12% 3/25 [00:00<00:02, 10.40it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  20% 5/25 [00:00<00:01, 11.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  28% 7/25 [00:00<00:01, 12.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  36% 9/25 [00:00<00:01, 13.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 14.44it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  52% 13/25 [00:00<00:00, 14.99it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 14.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  68% 17/25 [00:01<00:00, 15.02it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  76% 19/25 [00:01<00:00, 15.54it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  84% 21/25 [00:01<00:00, 16.25it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  92% 23/25 [00:01<00:00, 15.91it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-07-17 12:57:02 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.151 | nll_loss 3.391 | ppl 10.49 | wps 45170.1 | wpb 2731.6 | bsz 200 | num_updates 1840\n",
            "2021-07-17 12:57:02 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2021-07-17 12:57:02 | INFO | train | epoch 005 | loss 9.219 | nll_loss 3.948 | ppl 15.44 | wps 18018.1 | ups 4.85 | wpb 3714.7 | bsz 271.7 | num_updates 1840 | lr 0.00184001 | gnorm 0.214 | clip 0 | train_wall 74 | wall 368\n",
            "epoch 006:   0% 0/368 [00:00<?, ?it/s]2021-07-17 12:57:02 | INFO | fairseq.trainer | begin training epoch 6\n",
            "epoch 006: 100% 367/368 [01:14<00:00,  5.29it/s, loss=9.118, nll_loss=3.706, ppl=13.05, wps=18423.4, ups=4.88, wpb=3775.9, bsz=279.8, num_updates=2200, lr=0.00190693, gnorm=0.169, clip=0, train_wall=20, wall=441]2021-07-17 12:58:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.13it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  12% 3/25 [00:00<00:02,  7.54it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  20% 5/25 [00:00<00:02,  8.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  28% 7/25 [00:00<00:01, 10.46it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  36% 9/25 [00:00<00:01, 11.74it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  44% 11/25 [00:00<00:01, 12.67it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  52% 13/25 [00:00<00:00, 13.58it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  60% 15/25 [00:01<00:00, 13.55it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  68% 17/25 [00:01<00:00, 14.16it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  76% 19/25 [00:01<00:00, 14.66it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  84% 21/25 [00:01<00:00, 15.29it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  92% 23/25 [00:01<00:00, 15.32it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-07-17 12:58:18 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.066 | nll_loss 3.239 | ppl 9.44 | wps 44128.7 | wpb 2731.6 | bsz 200 | num_updates 2208\n",
            "2021-07-17 12:58:18 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2021-07-17 12:58:18 | INFO | train | epoch 006 | loss 9.144 | nll_loss 3.763 | ppl 13.57 | wps 17930.1 | ups 4.83 | wpb 3714.7 | bsz 271.7 | num_updates 2208 | lr 0.00190347 | gnorm 0.182 | clip 0 | train_wall 74 | wall 445\n",
            "epoch 007:   0% 0/368 [00:00<?, ?it/s]2021-07-17 12:58:18 | INFO | fairseq.trainer | begin training epoch 7\n",
            "epoch 007: 100% 367/368 [01:14<00:00,  4.73it/s, loss=9.055, nll_loss=3.545, ppl=11.67, wps=17959.8, ups=4.88, wpb=3678.8, bsz=268, num_updates=2500, lr=0.00178885, gnorm=0.152, clip=0, train_wall=20, wall=504]2021-07-17 12:59:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   4% 1/25 [00:00<00:02,  8.98it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  12% 3/25 [00:00<00:02, 10.41it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  20% 5/25 [00:00<00:01, 11.67it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  28% 7/25 [00:00<00:01, 12.92it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  36% 9/25 [00:00<00:01, 13.84it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 14.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  52% 13/25 [00:00<00:00, 14.86it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 14.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  68% 17/25 [00:01<00:00, 14.57it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  76% 19/25 [00:01<00:00, 15.09it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  84% 21/25 [00:01<00:00, 15.79it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  92% 23/25 [00:01<00:00, 15.60it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-07-17 12:59:34 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.002 | nll_loss 3.095 | ppl 8.55 | wps 44267.3 | wpb 2731.6 | bsz 200 | num_updates 2576\n",
            "2021-07-17 12:59:34 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2021-07-17 12:59:34 | INFO | train | epoch 007 | loss 9.054 | nll_loss 3.54 | ppl 11.63 | wps 17895.9 | ups 4.82 | wpb 3714.7 | bsz 271.7 | num_updates 2576 | lr 0.00176227 | gnorm 0.154 | clip 0 | train_wall 74 | wall 521\n",
            "epoch 008:   0% 0/368 [00:00<?, ?it/s]2021-07-17 12:59:34 | INFO | fairseq.trainer | begin training epoch 8\n",
            "epoch 008: 100% 367/368 [01:14<00:00,  5.08it/s, loss=8.981, nll_loss=3.363, ppl=10.29, wps=18559.3, ups=4.9, wpb=3788, bsz=270.8, num_updates=2900, lr=0.00166091, gnorm=0.136, clip=0, train_wall=20, wall=587]2021-07-17 13:00:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   4% 1/25 [00:00<00:02,  9.60it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  12% 3/25 [00:00<00:02, 10.89it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  20% 5/25 [00:00<00:01, 12.17it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  28% 7/25 [00:00<00:01, 13.27it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  36% 9/25 [00:00<00:01, 14.17it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 14.56it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  52% 13/25 [00:00<00:00, 14.86it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 14.39it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  68% 17/25 [00:01<00:00, 14.75it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  76% 19/25 [00:01<00:00, 15.10it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  84% 21/25 [00:01<00:00, 15.88it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  92% 23/25 [00:01<00:00, 15.87it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-07-17 13:00:51 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.968 | nll_loss 2.947 | ppl 7.71 | wps 44366.8 | wpb 2731.6 | bsz 200 | num_updates 2944\n",
            "2021-07-17 13:00:51 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2021-07-17 13:00:51 | INFO | train | epoch 008 | loss 8.986 | nll_loss 3.372 | ppl 10.35 | wps 17900.3 | ups 4.82 | wpb 3714.7 | bsz 271.7 | num_updates 2944 | lr 0.00164845 | gnorm 0.141 | clip 0 | train_wall 74 | wall 597\n",
            "epoch 009:   0% 0/368 [00:00<?, ?it/s]2021-07-17 13:00:51 | INFO | fairseq.trainer | begin training epoch 9\n",
            "epoch 009: 100% 367/368 [01:14<00:00,  5.18it/s, loss=8.929, nll_loss=3.235, ppl=9.42, wps=18395.3, ups=4.86, wpb=3781.6, bsz=279.5, num_updates=3300, lr=0.001557, gnorm=0.129, clip=0, train_wall=20, wall=670]2021-07-17 13:02:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   4% 1/25 [00:00<00:02,  9.62it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  12% 3/25 [00:00<00:01, 11.00it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  20% 5/25 [00:00<00:01, 12.25it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  28% 7/25 [00:00<00:01, 13.48it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  36% 9/25 [00:00<00:01, 14.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 14.76it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  52% 13/25 [00:00<00:00, 15.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 14.61it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  68% 17/25 [00:01<00:00, 14.86it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  76% 19/25 [00:01<00:00, 15.25it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  84% 21/25 [00:01<00:00, 15.85it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  92% 23/25 [00:01<00:00, 15.68it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-07-17 13:02:07 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.929 | nll_loss 2.839 | ppl 7.16 | wps 44734.9 | wpb 2731.6 | bsz 200 | num_updates 3312\n",
            "2021-07-17 13:02:07 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2021-07-17 13:02:07 | INFO | train | epoch 009 | loss 8.934 | nll_loss 3.243 | ppl 9.47 | wps 17915.2 | ups 4.82 | wpb 3714.7 | bsz 271.7 | num_updates 3312 | lr 0.00155417 | gnorm 0.132 | clip 0 | train_wall 74 | wall 674\n",
            "epoch 010:   0% 0/368 [00:00<?, ?it/s]2021-07-17 13:02:07 | INFO | fairseq.trainer | begin training epoch 10\n",
            "epoch 010: 100% 367/368 [01:14<00:00,  5.05it/s, loss=8.903, nll_loss=3.167, ppl=8.98, wps=18368.2, ups=4.89, wpb=3759.2, bsz=271.4, num_updates=3600, lr=0.00149071, gnorm=0.13, clip=0, train_wall=20, wall=732]2021-07-17 13:03:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   4% 1/25 [00:00<00:02,  9.24it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  12% 3/25 [00:00<00:02, 10.63it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  20% 5/25 [00:00<00:01, 11.79it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  28% 7/25 [00:00<00:01, 13.09it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  36% 9/25 [00:00<00:01, 14.05it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 14.49it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  52% 13/25 [00:00<00:00, 14.98it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 14.30it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  68% 17/25 [00:01<00:00, 14.69it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  76% 19/25 [00:01<00:00, 15.22it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  84% 21/25 [00:01<00:00, 15.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  92% 23/25 [00:01<00:00, 15.78it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-07-17 13:03:24 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.91 | nll_loss 2.75 | ppl 6.73 | wps 44502.3 | wpb 2731.6 | bsz 200 | num_updates 3680\n",
            "2021-07-17 13:03:24 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-07-17 13:03:27 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint10.pt (epoch 10 @ 3680 updates, score 8.91) (writing took 3.037295282000059 seconds)\n",
            "2021-07-17 13:03:27 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2021-07-17 13:03:27 | INFO | train | epoch 010 | loss 8.894 | nll_loss 3.144 | ppl 8.84 | wps 17171 | ups 4.62 | wpb 3714.7 | bsz 271.7 | num_updates 3680 | lr 0.00147442 | gnorm 0.128 | clip 0 | train_wall 74 | wall 753\n",
            "2021-07-17 13:03:27 | INFO | fairseq_cli.train | done training in 753.1 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMGSzREErEqV"
      },
      "source": [
        "テストデータで翻訳を行い，その性能を評価します．\n",
        "\n",
        "評価尺度BLEUで30ぐらい出ると思います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H989es1arIyX",
        "outputId": "0c00028c-f1bb-4101-850b-2006481ef7fc"
      },
      "source": [
        "! fairseq-interactive data-bin \\\n",
        "    --buffer-size 1024 \\\n",
        "    --batch-size 128 \\\n",
        "    --path checkpoints/checkpoint10.pt \\\n",
        "    --beam 5 \\\n",
        "    --lenpen 0.6 \\\n",
        "    < test.en \\\n",
        "    | grep '^H' \\\n",
        "    | cut -f 3 \\\n",
        "    | python src/decode.py \\\n",
        "    | tee output.txt \\\n",
        "    | sacrebleu corpus/test.ja"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 30.6 66.3/41.9/27.1/18.6 (BP = 0.891 ratio = 0.896 hyp_len = 45908 ref_len = 51230)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGx0a5Vxrlgb"
      },
      "source": [
        "翻訳結果と正解データの冒頭10文です．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7neQQskrNRl",
        "outputId": "df345d13-c64b-40e5-a3c4-c22de31c6e95"
      },
      "source": [
        "! head output.txt corpus/test.ja"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> output.txt <==\n",
            "彼女 の 母 は 美しい 女性 です 。\n",
            "畑 に は 6 人 の 羊 が い た 。\n",
            "今朝 は 散歩 の 間 に 大きな 写真 を 撮る こと が でき た 。\n",
            "今晩 電話 し ます 。\n",
            "彼女 は 本 の 虫 だ 。\n",
            "式 は 終わっ た 。\n",
            "私 は 彼 に 前日 自分 で 楽しん だ か どう か 尋ね た 。\n",
            "彼 は 注意 し て い ない 。\n",
            "列車 は いつも 定刻 に いる 。\n",
            "2 時 に 出発 すれ ば 、 彼 ら は 6 時 に 着く べき だ 。\n",
            "\n",
            "==> corpus/test.ja <==\n",
            "彼女 の お母さん は きれい な 女 の 人 です 。\n",
            "野原 に は 六 頭 の 羊 が い た 。\n",
            "この 朝 の 散歩 で とても 素晴らしい 写真 を 撮る こと が でき た 。\n",
            "今夜 電話 し ます 。\n",
            "彼女 は いわゆる 本 の 虫 です 。\n",
            "式典 は 終わり まし た 。\n",
            "昨日 は 楽しかっ た か 、 と 私 は 彼 に 尋ね た 。\n",
            "彼 に は 心配 事 が ない 。\n",
            "その 列車 は いつも 時刻 通り だ 。\n",
            "2 時 に 出発 し たら 、 6 時 に は つく はず だ 。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXH6CjmZrzKH"
      },
      "source": [
        "たのしい"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ldTDx2FrjLp"
      },
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from sacremoses import MosesTokenizer\n",
        "import sentencepiece as spm\n",
        "from fairseq.models.transformer import TransformerModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jg6rlaqr9Ax"
      },
      "source": [
        "mt = MosesTokenizer(lang = 'en')\n",
        "sp = spm.SentencePieceProcessor(model_file='bpe.model')\n",
        "model = TransformerModel.from_pretrained('./checkpoints/', checkpoint_file='checkpoint10.pt', data_name_or_path='data-bin')\n",
        "\n",
        "def preproc_en(x):\n",
        "  x = unicodedata.normalize('NFKC', x)\n",
        "  x = re.sub(mt.AGGRESSIVE_HYPHEN_SPLIT[0], r'\\1 - ', x)\n",
        "  x = mt.tokenize(x, escape = False)\n",
        "  x = ' '.join(x)\n",
        "  x = x.lower()\n",
        "  x = ' '.join(sp.encode(x, out_type = 'str'))\n",
        "  return x\n",
        "\n",
        "def translate(x):\n",
        "  x = preproc_en(x)\n",
        "  x = model.translate(x, beam = 5, lenpen = 0.6)\n",
        "  x = ''.join(x.split()).replace('▁', '').strip()\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZiI1ffvswHT",
        "outputId": "a288a431-7979-456d-ac23-80a0e37636b2"
      },
      "source": [
        "while True:\n",
        "  x = input('英文を入力 > ')\n",
        "  if not x:\n",
        "    break\n",
        "  x = translate(x)\n",
        "  print('翻訳結果 > ' + x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "英文を入力 > Why is the baby crying?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "翻訳結果 > 赤ん坊は何で泣いているの?\n",
            "英文を入力 > There was no one left but me.\n",
            "翻訳結果 > 私には誰も残っていなかった。\n",
            "英文を入力 > I know how old you are.\n",
            "翻訳結果 > あなたが何歳か知っている。\n",
            "英文を入力 > \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SH9HQ5mszTE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}